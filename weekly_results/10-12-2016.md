By running the entire dataset (2.5 million lines) using 2/3 for training and 1/3 for testing on a random forest, I got the following results:

```
0 means failed builds, 1 passing builds.

Clasification report:
             precision    recall  f1-score   support

          0       0.97      0.95      0.96    265179
          1       0.98      0.99      0.98    606294

avg / total       0.98      0.98      0.98    871473


Confussion matrix:
[[251445  13734]
 [  7719 598575]]
```

Then I ran the same method, but instead of using the entire dataset, I ran it for each project. Below are the f-scores for the biggest projects:


<div>
    <a href="https://plot.ly/~gferrei/0/" target="_blank" title="" style="display: block; text-align: center;"><img src="https://plot.ly/~gferrei/0.png" alt="" style="max-width: 100%;width: 600px;"  width="600" onerror="this.onerror=null;this.src='https://plot.ly/404.png';" /></a>
    <script data-plotly="gferrei:0"  src="https://plot.ly/embed.js" async></script>
</div>
